{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5668aba-f425-4eb0-a151-018656fdb7df",
   "metadata": {},
   "source": [
    "# Analyse Customer Behaviour in a Multi Category e-Commerce Website\n",
    "The dataset contains customer behaviour data of a large multi category e-commerce website. The customer behaviour is reflected in the `event_type` field which is either view, cart or purchase. Each row in the file represents an event. All events are related to products and users. Each event is like many-to-many relation between products and users. This exercise uses the 2019 October dataset published in Kaggle https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store. The dataset originally collected from Open CDP https://rees46.com/en/open-cdp. \n",
    "\n",
    "The selected dataset is approximately 5Gb in volume which makes data processing a difficult in our usual RStudio or Colab environment. Therefore, we need to move into a big data technology to process this dataset. In this exercise, we run the exerecise in a Spark Cluster run on a Cloud environment. We use Jupyter Notebook as our IDE and connect to the Spark Cluster using Python (`pyspark`). The exercise first connect to the Spark cluster, extract the data which is stored in HDFS and then run several descriptive analytics to understand the customer behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243aab86-6ccf-47f5-83d0-cf11b084df03",
   "metadata": {},
   "source": [
    "## 1. Spark configuration in `pyspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d367c5-aeeb-4c28-9b07-306ab4c88659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 13:38:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-98b0-m.us-central1-b.c.boffin-389410.internal:43525\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f020e4c6d70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder.master(\"localhost\") \\\n",
    ".config(\"spark.driver.port\",\"7077\") \\\n",
    ".config(\"spark.executor.memory\", '3g') \\\n",
    ".getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b31ba-22f2-4afc-b34e-f93d841af488",
   "metadata": {},
   "source": [
    "## 2. Create schema and read data\n",
    "It is recommended to create a Spark schema before read data to a Spark dataframe. A schema validates the type of data fields we import from HDFS and whether the data point is nullable or not. You can find the available Spark data types on https://spark.apache.org/docs/latest/sql-ref-datatypes.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c2f80e-7ad3-403a-b35f-921ddd671372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "schema = StructType().add('event_time', TimestampType(),True) \\\n",
    "                    .add('event_type',StringType(),True) \\\n",
    "                    .add('product_id', IntegerType(),True) \\\n",
    "                    .add('category_id', StringType(),True) \\\n",
    "                    .add('category_code', StringType(), True) \\\n",
    "                    .add('brand', StringType(),True) \\\n",
    "                    .add('price', FloatType(),True) \\\n",
    "                    .add('user_id', IntegerType(),True) \\\n",
    "                    .add('user_session', StringType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e988916-15c7-4c9f-8fa4-8e7c473cf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "behaviour = spark.read.schema(schema).csv('hdfs:///data/behaviour_oct.csv', header=True, \n",
    "                            multiLine=True, escape=\"\\\"\")\n",
    "behaviour.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d885c7-d2b4-4b12-b5cf-0e5b0971ef4f",
   "metadata": {},
   "source": [
    "## 3. Check the dataset\n",
    "You can count the total number of records and display first few records to check whether Spark dataframe is successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12600dd-07e9-4b4c-9c50-538dd8543aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42448764"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviour.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7109bd3-bb07-4789-88ee-69fbc4be42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d72291-3dd4-4189-bc41-39b8d25c0936",
   "metadata": {},
   "source": [
    "## 4. Run `pyspark` query \n",
    "Spark dataframe has number of data manipulation methods available to run queries. These methods interprets these methods to Spark SQL queries. Query results can be converted to pandas dataframe which allows you to process the query result as a python pandas dataframe.\n",
    "\n",
    "The following query counts the number of records by `event_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca76a1-6112-468f-a7d5-bb332ab4f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "event_type = behaviour.groupby('event_type').count().toPandas()\n",
    "event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec84fa-4315-4c9c-b328-ba37d15a56bc",
   "metadata": {},
   "source": [
    "## Exercise 01\n",
    "Using the Spark dataframe, run the following queries.\n",
    "\n",
    "    1. Total value of customer behaviour by event type.\n",
    "    2. Top 10 brands purchased by value\n",
    "    3. Top 10 brands purchased by volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac88226-e6a9-4be4-8aa2-18bce9e6b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_value = behaviour.groupby('event_type').agg({'price': 'sum'}).toPandas()\n",
    "event_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc97e1-7baf-4216-b7ad-6d6caf4f2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_brands_by_value = behaviour.filter('event_type = \"purchase\"').groupby('brand').agg({'price': 'sum'}).toPandas()\n",
    "top_brands_by_value.sort_values(['sum(price)'],ascending=[False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacaaa6-8f94-48c4-b6af-996013f962be",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_brands_by_volume = behaviour.filter('event_type = \"purchase\"').groupby('brand').agg({'product_id': 'count'}).toPandas()\n",
    "top_brands_by_volume.sort_values(['count(product_id)'],ascending=[False]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78076ea8-163d-476e-bf21-3026753f6a38",
   "metadata": {},
   "source": [
    "## Exercise 02\n",
    "\n",
    "The `category_code` variable includes the product category and sub-categories deliemeted by a `.`. Using a string manipulation, extract the following query results.\n",
    "\n",
    "1. Extract the main category of the purchased items with the price\n",
    "2. Total value and volume of purchased items by the main category\n",
    "3. Top 10 product categories by value\n",
    "4. Top 10 product categories by volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b283cc-1587-4697-b293-304e5462f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "categories = behaviour.filter('event_type = \"purchase\"').select(split('category_code','\\.').getItem(1).alias('categories'), 'product_id', 'price').toPandas()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6009f1-1d10-4d83-8f92-5f8666a895a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "category_master = categories.groupby('categories').agg(Value=('price',np.sum), Volume=('product_id',np.count_nonzero))\n",
    "category_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca161a-d201-4072-84dc-a9e216c70de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_master.sort_values(['Value'],ascending=[False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b0417-1fdd-4716-967b-46c090a6f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_master.sort_values(['Volume'],ascending=[False]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3b258-182e-46fd-a2e3-9aa2f3363fc0",
   "metadata": {},
   "source": [
    "## Exercise 03\n",
    "The dataset includes items viewed by the users. This can be identified using the `event_type`. The company wants to analyse the daily view pattern during the month. First we need to generate the date from the timestamp value. Then the company requires us to generate the following query results.\n",
    "\n",
    "1. Create daily view pattern of apple products\n",
    "2. Visualise the timeseries using a line chart\n",
    "3. Compare the view patterns of apple vs samsung\n",
    "4. Compare the two frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94e43b-2aa2-4782-8e5c-c8bbef47878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "views = behaviour.withColumn('date', to_date('event_time'))\n",
    "views.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4c42a-040e-4bfe-a626-1ef55860be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_by_brand = views.groupby('brand','date').count()\n",
    "apple_views = views_by_brand.filter('brand=\"apple\"').drop('brand').toPandas().sort_values(['date'])\n",
    "apple_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58869378-01df-433e-bb81-75be72e69282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(apple_views,x=\"date\",y=\"count\")\n",
    "sns.set(rc={'figure.figsize':(20,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b96f37-6564-4a11-be2f-e7e54d070b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_vs_samsung = views_by_brand.filter('brand=\"apple\" OR brand=\"samsung\"').toPandas().sort_values(['date'])\n",
    "apple_vs_samsung.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366f7d1-b7ba-4247-8fa4-3d352207eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(apple_vs_samsung,x=\"date\",y=\"count\", hue=\"brand\")\n",
    "sns.set(rc={'figure.figsize':(30, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbac48-fa9e-42fb-8065-e9c27faf5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(apple_vs_samsung, x=\"count\", hue=\"brand\", element=\"step\")\n",
    "sns.set(rc={'figure.figsize':(40,20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d85e8-8a59-46fb-a16a-9bbd52caf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(apple_vs_samsung, x=\"count\", hue=\"brand\", kind='kde', multiple=\"stack\")\n",
    "sns.set(rc={'figure.figsize':(40,20)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4015c51-410a-4ace-b0be-226b9a224031",
   "metadata": {},
   "source": [
    "## 5. Close Spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03641d14-d5dc-4d6a-9078-7b54a91147e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 13:42:56 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=133; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-351061576575-hc5whhgs/a135ab3a-2555-406c-8a00-67662c732070/spark-job-history/application_1689946506511_0002.inprogress\n",
      "23/07/21 13:42:56 WARN GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=219; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://dataproc-temp-us-central1-351061576575-hc5whhgs/a135ab3a-2555-406c-8a00-67662c732070/spark-job-history/application_1689946506511_0002.inprogress -> gs://dataproc-temp-us-central1-351061576575-hc5whhgs/a135ab3a-2555-406c-8a00-67662c732070/spark-job-history/application_1689946506511_0002)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd31b4-3d9a-4175-9767-032187d661da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
